{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a collection of webscrapers used to get data we need from registries\n",
    "\n",
    "For now, this is only certain registries as others have so few trials they can be easily manually screened.\n",
    "\n",
    "This doesn't really need to be a notebook but for organisation, disply, and if you are running than all at once, ease. Any of the specific scrapers could be copied into a .py file and run that way if preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add for July scrape:\n",
    "\n",
    "-Contact info\n",
    "-last updated date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent = str(Path(cwd).parents[0])\n",
    "sys.path.append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import post\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_url(url, parse='html'):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    response = get(url, verify = False, headers=headers)\n",
    "    html = response.content\n",
    "    if parse == 'html':\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    elif parse == 'xml':\n",
    "        soup = BeautifulSoup(html, \"xml\")\n",
    "    return soup\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "save_path = parent + '/data/raw_scraping_output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trials these were run on are those in the ICTRP dataset after the initial inclusions/exclusions were made (observational, pre-2020 trials, trials that are withdrawn/cancelled). This code assumes you read in that dataset to a DataFrame below and work from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(parent + '/data/scrape_df_jul21.csv')\n",
    "df.source_register.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClinicalTrials.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nct_urls = df[df.source_register == 'ClinicalTrials.gov'].web_address.to_list()\n",
    "pubmed_res_str = 'Publications automatically indexed to this study by ClinicalTrials.gov Identifier (NCT Number):'\n",
    "\n",
    "ctgov_list = []\n",
    "\n",
    "\n",
    "for nct in tqdm(nct_urls):\n",
    "    soup = get_url(nct)\n",
    "    trial_dict = {}\n",
    "    \n",
    "    trial_dict['trial_id'] = nct[-11:]\n",
    "    \n",
    "    #Completion Dates\n",
    "    if soup.find('span', {'data-term': \"Primary Completion Date\"}):\n",
    "        trial_dict['pcd'] = soup.find('span', {'data-term': \"Primary Completion Date\"}).find_next('td').text\n",
    "    else:\n",
    "        trial_dict['pcd'] = None\n",
    "    if soup.find('span', {'data-term': \"Study Completion Date\"}):\n",
    "        trial_dict['scd'] = soup.find('span', {'data-term': \"Study Completion Date\"}).find_next('td').text\n",
    "    else:\n",
    "        trial_dict['scd'] = None\n",
    "\n",
    "    #Tabular Results Status\n",
    "    if soup.find('li', {'id':'results'}):\n",
    "        trial_dict['tab_results'] = soup.find('li', {'id':'results'}).text.strip()\n",
    "    else:\n",
    "        trial_dict['tab_results'] = None\n",
    "\n",
    "    #Auto-linked results via PubMed\n",
    "    if soup.find('span', text=pubmed_res_str):\n",
    "        pm_linked = []\n",
    "        for x in soup.find('span', text=pubmed_res_str).find_next('div').find_all('div'):\n",
    "            pm_linked.append(x.text.strip())\n",
    "        trial_dict['linked_pubs'] = pm_linked\n",
    "    else:\n",
    "        trial_dict['linked_pubs'] = None\n",
    "\n",
    "    #Results citations provided by sponsor\n",
    "    if soup.find('span', text='Publications of Results:'):\n",
    "        spon_pubs = []\n",
    "        for x in soup.find('span', text='Publications of Results:').find_next('div').find_all('div'):\n",
    "            spon_pubs.append(x.text.strip())\n",
    "        trial_dict['spon_pubs'] = spon_pubs\n",
    "    else:\n",
    "        trial_dict['spon_pubs'] = None\n",
    "\n",
    "    #Trial Status:\n",
    "    if soup.find('span', {'data-term': 'Recruitment Status'}):\n",
    "        trial_dict['trial_status'] = soup.find('span', {'data-term': 'Recruitment Status'}).next_sibling.replace(':','').strip()\n",
    "    else:\n",
    "        trial_dict['trial_status'] = None\n",
    "\n",
    "    #Secondary IDs:\n",
    "    if soup.find('td', text='Other Study ID Numbers:'):\n",
    "        ids = []\n",
    "        for a in soup.find('td', text='Other Study ID Numbers:').find_next('td').text.split('\\n'):\n",
    "            if a.strip():\n",
    "                ids.append(a.strip())\n",
    "        trial_dict['secondary_ids'] = ids\n",
    "        \n",
    "    #Last updated date:\n",
    "    if soup.find('span', {'data-term': 'Last Update Posted'}):\n",
    "        trial_dict['last_updated'] = soup.find('span', {'data-term': 'Last Update Posted'}).next_sibling.replace(':','').strip()\n",
    "    else:\n",
    "        trial_dict['last_updated'] = None\n",
    "        \n",
    "    emails = []\n",
    "    for x in soup.select('a[href^=mailto]'):\n",
    "        emails.append(x.text)\n",
    "    trial_dict['emails'] = emails\n",
    "    \n",
    "    ctgov_list.append(trial_dict)\n",
    "    \n",
    "#Can be expanded for some covariates as needed but also can archive our full copy from the FDAAA TT \n",
    "#on the day of the scrape and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgov_results = pd.DataFrame(ctgov_list)\n",
    "\n",
    "ctgov_results['pcd'] = pd.to_datetime(ctgov_results['pcd'])\n",
    "ctgov_results['scd'] = pd.to_datetime(ctgov_results['scd'])\n",
    "ctgov_results['last_updated'] = pd.to_datetime(ctgov_results['last_updated'])\n",
    "\n",
    "ctgov_results.to_csv(save_path + 'ctgov_results_11jul2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISRCTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isrctn_ids = df[df.source_register == 'ISRCTN'].trialid.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISRCTN\n",
    "\n",
    "isrctn_urls = df[df.source_register == 'ISRCTN'].trialid.to_list()\n",
    "\n",
    "isrctn_list = []\n",
    "\n",
    "for i in tqdm(isrctn_urls[0:10]):\n",
    "\n",
    "    trial_dict = {}\n",
    "    soup = get_url(f'https://www.isrctn.com/{i}')\n",
    "\n",
    "    #Trial ID\n",
    "    trial_dict['trial_id'] = i\n",
    "\n",
    "    #Trial Status\n",
    "    if soup.find('dt', text='Overall trial status'):\n",
    "        trial_dict['overall_status'] = soup.find('dt', text='Overall trial status').find_next().text.strip()\n",
    "    else:\n",
    "        trial_dict['overall_status'] = None\n",
    "        \n",
    "    trial_dict['recruitment_status'] = soup.find('dt', text='Recruitment status').find_next().text.strip()\n",
    "\n",
    "    #Dates\n",
    "    trial_dict['trial_start_date'] = soup.find('h3', text='Overall trial start date').find_next().text.strip()\n",
    "\n",
    "    #Dates\n",
    "    trial_dict['trial_end_date'] = soup.find('h3', text='Overall trial end date').find_next().text.strip()\n",
    "    \n",
    "    #Other IDs\n",
    "    sid_dict={}\n",
    "    if soup.find('h3', text='EudraCT number').find_next().text.strip():\n",
    "        sid_dict['eudract_number'] = soup.find('h3', text='EudraCT number').find_next().text.strip()\n",
    "    else:\n",
    "        sid_dict['eudract_number'] = None\n",
    "    if soup.find('h3', text='ClinicalTrials.gov number').find_next().text.strip():\n",
    "        sid_dict['ctgov_number'] = soup.find('h3', text='ClinicalTrials.gov number').find_next().text.strip()\n",
    "    else:\n",
    "        sid_dict['ctgov_number'] = None\n",
    "    if soup.find('h3', text='Protocol/serial number').find_next().text.strip():\n",
    "        sid_dict['other_id'] = soup.find('h3', text='Protocol/serial number').find_next().text.strip()\n",
    "    else:\n",
    "        sid_dict['other_id'] = None\n",
    "    trial_dict['secondary_ids'] = sid_dict\n",
    "\n",
    "    #Results stuff\n",
    "    if soup.find('h3', text='Basic results (scientific)').find_next():\n",
    "        trial_dict['basic_results'] = soup.find('h3', text='Basic results (scientific)').find_next().text.strip()\n",
    "    else:\n",
    "        trial_dict['basic_results'] = None\n",
    "    if soup.find('h3', text='Publication list').find_next().text.strip():\n",
    "        trial_dict['pub_list'] = soup.find('h3', text='Publication list').find_next().text.strip()\n",
    "    else:\n",
    "        trial_dict['pub_list'] = None\n",
    "        \n",
    "    if soup.find('dt', text='Last edited'):\n",
    "        trial_dict['last_updated'] = soup.find('dt', text='Overall trial status').find_next().text.strip()\n",
    "    else:\n",
    "        trial_dict['last_updated'] = None\n",
    "    \n",
    "    emails = []\n",
    "    for x in soup.select('a[href^=mailto]'):\n",
    "        emails.append(x.text)\n",
    "    trial_dict['emails'] = emails\n",
    "    \n",
    "    isrctn_list.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(isrctn_list).to_csv(save_path + 'isrctn_11jul_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above doesn't work great anymore because ISRCTN switched to an API but easier to just download the \n",
    "#full isrctn and just filter it and grab the IDs we want than fiddle with the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isrctn_data = pd.read_csv(parent + '/data/registry_data/ISRCTN search results_11july.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isrctn_data[isrctn_data.ISRCTN.isin(isrctn_ids)].to_csv(save_path + 'isrctn_11jul_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EUCTR\n",
    "IDs will be injested in form of EUCTR2020-000890-25-FR\n",
    "We need to kill the EUCTR and the -FR part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euctr_urls = df[df.source_register == 'EU Clinical Trials Register'].web_address.to_list()\n",
    "\n",
    "euctr_ids = df[df.source_register == 'EU Clinical Trials Register'].trialid.to_list()\n",
    "\n",
    "euctr_trials = []\n",
    "\n",
    "for i in tqdm(euctr_ids):\n",
    "\n",
    "    euctr_id = re.search('(.*\\d)', i.replace('EUCTR',''))[0]\n",
    "\n",
    "    search_url = 'https://www.clinicaltrialsregister.eu/ctr-search/search?query={}'\n",
    "    #First blank is the trial number, 2nd is the abbreviation for the protocol country\n",
    "    protocol_url = 'https://www.clinicaltrialsregister.eu/ctr-search/trial/{}/{}'\n",
    "\n",
    "    soup=get_url(search_url.format(euctr_id))\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    #trial id\n",
    "    trial_dict['trial_id'] = euctr_id\n",
    "\n",
    "    #Results status\n",
    "    trial_dict['results_status'] = soup.find('span', {'class':'label'}, text='Trial results:').find_next().text\n",
    "\n",
    "    #Countries\n",
    "    country_list = []\n",
    "    for x in soup.find('span', text='Trial protocol:').parent.find_all('a'):\n",
    "        country_list.append(x.text)\n",
    "    trial_dict['countries'] = country_list\n",
    "\n",
    "    #Individual Protocol Data\n",
    "    #Completion dates\n",
    "    comp_dates = []\n",
    "    status_list = []\n",
    "    emails = []\n",
    "    for c in country_list:\n",
    "        psoup = get_url(protocol_url.format(euctr_id, c))\n",
    "        if psoup.find('td', text='Date of the global end of the trial'):\n",
    "            comp_dates.append(psoup.find('td', text='Date of the global end of the trial').find_next().text)\n",
    "        else:\n",
    "            comp_dates.append(None)\n",
    "    \n",
    "    #Trial status\n",
    "        if psoup.find('td', text='Trial Status:'):\n",
    "            status_list.append(psoup.find('td', text='Trial Status:').find_next('td').text.strip())\n",
    "        else:\n",
    "            status_list.append(None)\n",
    "\n",
    "    #secondary_ids\n",
    "        sid_dict = {}\n",
    "        if psoup.find('td', text='ISRCTN (International Standard Randomised Controlled Trial) Number'):\n",
    "            sid_dict['isrctn'] = psoup.find('td', text='ISRCTN (International Standard Randomised Controlled Trial) Number').find_next().text.strip()\n",
    "        if psoup.find('td', text='US NCT (ClinicalTrials.gov registry) number'):\n",
    "            sid_dict['nct_id'] = psoup.find('td', text='US NCT (ClinicalTrials.gov registry) number').find_next().text.strip()\n",
    "        if psoup.find('td', text=\"Sponsor's protocol code number\"):\n",
    "            sid_dict['spon_id'] = psoup.find('td', text=\"Sponsor's protocol code number\").find_next().text.strip()\n",
    "            \n",
    "    #emails\n",
    "        if psoup.find('td', text='E-mail'):\n",
    "            for x in psoup.find_all('td', text='E-mail'):\n",
    "                emails.append(x.find_next().text)\n",
    "            \n",
    "    \n",
    "    trial_dict['global_trial_end_dates'] =  comp_dates\n",
    "    trial_dict['status_list'] = status_list\n",
    "    trial_dict['emails'] = emails\n",
    "    if len(sid_dict) > 0:\n",
    "        trial_dict['secondary_ids'] = sid_dict\n",
    "    else:\n",
    "        trial_dict['secondary_ids'] = None\n",
    "    \n",
    "    euctr_trials.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(euctr_trials).to_csv(save_path + 'euctr_12jul_2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRKS\n",
    "We can get DRKS trials via the URL in the ICTRP dataset like:\n",
    "https://www.drks.de/drks_web/navigate.do?navigationId=trial.HTML&TRIAL_ID=DRKS00021186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_it_exist(soup, element, e_class, n_e=False):\n",
    "    if not n_e:\n",
    "        location = soup.find(element, class_=e_class).text.strip()\n",
    "    elif n_e:\n",
    "        location = soup.find(element, class_=e_class).next_element.next_element.next_element.next_element.strip()\n",
    "    if location == '[---]*':\n",
    "        field = None\n",
    "    else:\n",
    "        field = location\n",
    "    return field\n",
    "\n",
    "drks_urls = df[df.source_register == 'German Clinical Trials Register'].web_address.to_list()\n",
    "\n",
    "drks_trials = []\n",
    "\n",
    "for d in tqdm(drks_urls): \n",
    "    \n",
    "    soup = get_url(d)\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    trial_dict['trial_id'] = soup.find('li', class_='drksId').next_element.next_element.next_element.next_element.strip()\n",
    "\n",
    "    history = get_url(f'https://www.drks.de/drks_web/navigate.do?navigationId=trial.history&TRIAL_ID={trial_dict[\"trial_id\"]}')\n",
    "    \n",
    "    st_class = ['state', 'deadline']\n",
    "    st_labels = ['recruitment_status', 'study_closing_date']\n",
    "    for lab, s_class in zip(st_labels, st_class):\n",
    "        trial_dict[lab] = does_it_exist(soup, 'li', s_class, n_e=True)\n",
    "\n",
    "    s_id_list = []\n",
    "    ul = soup.find('ul', class_='secondaryIDs').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        s_id_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            s_id_dict = {}\n",
    "            s_id_dict['id_type'] = u.next_element.next_element.next_element.strip().replace(':','')\n",
    "            li_t = u.next_element.next_element.next_element.next_element.strip()\n",
    "            li_t = re.sub('\\n', '|', li_t)\n",
    "            li_t = re.sub('\\s+', '', li_t).replace('(','').replace(')','')\n",
    "            id_info = li_t.split('|')\n",
    "            if len(id_info) > 1:   \n",
    "                s_id_dict['registry'] = id_info[1]\n",
    "                s_id_dict['secondary_id'] = id_info[0]\n",
    "            else:\n",
    "                s_id_dict['registry'] = None\n",
    "                s_id_dict['secondary_id'] = id_info[0]\n",
    "            s_id_list.append(s_id_dict)\n",
    "\n",
    "    trial_dict['secondary_ids'] = s_id_list\n",
    "\n",
    "    docs_list = []\n",
    "    ul = soup.find('ul', class_='publications').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        docs_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            doc_dict = {}\n",
    "            doc_dict['document_type'] = u.next_element.next_element.next_element.strip().replace(':','')\n",
    "            if u.find('a'):\n",
    "                doc_dict['link_to_document'] = u.find('a').get('href')\n",
    "            else:\n",
    "                doc_dict['link_to_document'] = None\n",
    "            docs_list.append(doc_dict)\n",
    "    trial_dict['results_publications_documents'] = docs_list\n",
    "    \n",
    "    trial_dict['last_updated'] = pd.to_datetime(history.find('tbody').find_next('td').text, format='%m-%d-%Y')\n",
    "    \n",
    "    emails = []\n",
    "    for x in soup.select('a[href^=mailto]'):\n",
    "        emails.append(x.text.replace(' at ', '@'))\n",
    "    trial_dict['emails'] = emails\n",
    "    \n",
    "    drks_trials.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(drks_trials).to_csv(save_path + 'drks_trials_12jul_2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTRI\n",
    "\n",
    "We can get right to a trial registration with a URL from the ICTRP like:\n",
    "http://www.ctri.nic.in/Clinicaltrials/pmaindet2.php?trialid=43553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctri_urls = df[df.source_register == 'CTRI'].web_address.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need a slightly more robust function to fetch trial data from the CTRI\n",
    "def get_ctri(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    tries=3\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            response = get(url, verify = False, headers=headers)\n",
    "        except ConnectionError as e:\n",
    "            if i < tries - 1:\n",
    "                sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctri_urls[406]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctri_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctri_list = []\n",
    "\n",
    "for c in tqdm(ctri_urls):\n",
    "\n",
    "    soup = get_ctri(c)\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    trial_dict['trial_id'] = soup.find('td', text = re.compile('CTRI Number\\s*')).find_next('b').find_next('b').text.strip()\n",
    "    \n",
    "    trial_dict['completion_date_india'] = soup.find('td', text = 'Date of Study Completion (India)').find_next('td').text.strip()\n",
    "\n",
    "    trial_dict['completion_date_global'] = soup.find('td', text = 'Date of Study Completion (Global)').find_next('td').text.strip()\n",
    "\n",
    "    trial_dict['recruitment_status_india'] = soup.find('b', text='Recruitment Status of Trial (India)').find_next('td').text.strip()\n",
    "\n",
    "    trial_dict['recruitment_status_global'] = soup.find('b', text='Recruitment Status of Trial (Global)').find_next('td').text.strip()\n",
    "\n",
    "    trial_dict['publication_details'] = soup.find('b', text='Publication Details').find_next('td').text.strip()\n",
    "\n",
    "    if soup.find('b', text = re.compile('Secondary IDs if Any')):\n",
    "        trial_dict['secondary_ids'] = soup.find('b', text = re.compile('Secondary IDs if Any')).parent.parent.find_all('tr')\n",
    "    else:\n",
    "        trial_dict['secondary_ids'] = None\n",
    "\n",
    "    trial_dict['last_updated'] = soup.find('b', text='Last Modified On:').find_next('td').text.strip()\n",
    "    \n",
    "    emails = []\n",
    "    for x in soup.find_all('td', text='EmailÂ '):\n",
    "        emails.append(x.find_next('td').text.strip())\n",
    "    trial_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    ctri_list.append(trial_dict)\n",
    "    from time import sleep\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ctri_list).to_csv(save_path + 'ctri_12jul2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANZCTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anzctr_urls = df[df.source_register == 'ANZCTR'].web_address.to_list()\n",
    "\n",
    "anzctr_trials = []\n",
    "\n",
    "for u in tqdm(anzctr_urls):\n",
    "\n",
    "    soup = get_url(u)\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    trial_dict['trial_id'] = soup.find('span', {'id': 'ctl00_body_CXACTRNUMBER'}).text.replace('p','')\n",
    "\n",
    "    trial_dict['last_updated'] = soup.find('span', {'id': 'ctl00_body_CXUPDATEDATE'}).text\n",
    "\n",
    "    trial_dict['trial_status'] = soup.find('span', {'id': 'ctl00_body_CXRECRUITMENTSTATUS'}).text\n",
    "\n",
    "    anticipated_end_date = soup.find('span', {'id': 'ctl00_body_CXANTICIPATEDENDDATE'}).text\n",
    "\n",
    "    actual_end_date = soup.find('span', {'id': 'ctl00_body_CXACTUALENDDATE'}).text\n",
    "\n",
    "    if anticipated_end_date:\n",
    "        trial_dict['completion_date'] = anticipated_end_date\n",
    "    else:\n",
    "        trial_dict['completion_date'] = actual_end_date\n",
    "\n",
    "    secondary_ids = []\n",
    "\n",
    "    for x in soup.find_all('span', text=re.compile('Secondary ID \\[\\d*\\]')):\n",
    "        secondary_ids.append(x.find_next('div').span.text.strip())\n",
    "\n",
    "    trial_dict['secondary_ids'] = secondary_ids\n",
    "\n",
    "    results_dict = {}\n",
    "\n",
    "    if soup.find('div', {'id': 'ctl00_body_divNoResultsANZCTR'}):\n",
    "        trial_dict['results'] = None\n",
    "    else:\n",
    "        citations = []\n",
    "        for x in soup.find_all('span', text=re.compile('Publication date and citation/details \\[\\d*\\]')):\n",
    "            citations.append(x.find_next('div').span.text)\n",
    "\n",
    "        results_dict['citations'] = citations\n",
    "\n",
    "        if soup.find('a', {'id': 'ctl00_body_hyperlink_CXRESULTATTACHMENT'}):\n",
    "            results_dict['basic_reporting_doc'] = soup.find('a', {'id': 'ctl00_body_hyperlink_CXRESULTATTACHMENT'}).text\n",
    "\n",
    "    trial_dict['results'] = results_dict\n",
    "    \n",
    "    trial_dict['last_updated'] = pd.to_datetime(soup.find('span', {'id': 'ctl00_body_CXUPDATEDATE'}).text, format='%d/%m/%Y')\n",
    "    \n",
    "    emails = []\n",
    "    for x in soup.select('span[id$=_CXEMAIL]'):\n",
    "        if x.text and 'Email [' not in x.text:\n",
    "            emails.append(x.text)\n",
    "    trial_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    anzctr_trials.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anzctr_df = pd.DataFrame(anzctr_trials)\n",
    "anzctr_df.to_csv(save_path + 'anzctr_trials_12jul2021.csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Easiest to just to this query and then filter\n",
    "\n",
    "rsp = post(\n",
    "    \"https://api.trialregister.nl/trials/public.trials/_msearch?\",\n",
    "    headers={\"Content-Type\": \"application/x-ndjson\", \"Accept\": \"application/json\"},\n",
    "    data='{\"preference\":\"results\"}\\n{\"query\":{\"match_all\":{}},\"size\":10000,\"_source\":{\"includes\":[\"*\"],\"excludes\":[]},\"sort\":[{\"id\":{\"order\":\"desc\"}}]}\\n',\n",
    ")\n",
    "results = rsp.json()\n",
    "hits = results[\"responses\"][0][\"hits\"][\"hits\"]\n",
    "records = [hit[\"_source\"] for hit in hits]\n",
    "\n",
    "all_keys = set().union(*(record.keys() for record in records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(all_keys)\n",
    "\n",
    "from datetime import date\n",
    "import csv\n",
    "\n",
    "def ntr_csv(save_path):\n",
    "    with open(save_path + 'ntr - ' + str(date.today()) + '.csv','w', newline = '', encoding='utf-8') as ntr_csv:\n",
    "        writer=csv.DictWriter(ntr_csv,fieldnames=labels)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only what we need from the NTR\n",
    "\n",
    "ntr_ids = df[df.source_register == 'Netherlands Trial Register'].trialid.to_list()\n",
    "\n",
    "ntr_df = pd.read_csv(save_path + 'ntr - 2021-07-12.csv')\n",
    "\n",
    "covid_ntr = ntr_df[ntr_df.idFull.isin(ntr_ids)]\n",
    "\n",
    "covid_ntr[['idFull', 'status', 'dateStop', 'publications', 'idSource', 'isrctn', 'contact']].to_csv(save_path + 'ntr_covid_12jul.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = get_url('https://www.irct.ir/trial/53653')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in s.find_all('strong', text='Email'):\n",
    "    print(x.find_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\d{4}-\\d{2}-\\d{2}', s.find('span', text='Last update:').find_next().text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "trial_dict['last_updated'] = re.findall(r'\\d{4}-\\d{2}-\\d{2}', soup.find('span', text='Last update:').find_next().text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irct_urls = df[df.source_register == 'IRCT'].web_address.to_list()\n",
    "\n",
    "irct_list = []\n",
    "\n",
    "for url in tqdm(irct_urls):\n",
    "\n",
    "    soup=get_url(url)\n",
    "\n",
    "    trial_dict = {}\n",
    "    \n",
    "    if soup.find('span', text='IRCT registration number:'):\n",
    "        trial_dict['trial_id'] = soup.find('span', text='IRCT registration number:').find_next('strong').text.strip()\n",
    "    else:\n",
    "        trial_dict['trial_id'] = None\n",
    "\n",
    "    if soup.find('dt', text=re.compile('\\sRecruitment status\\s')):\n",
    "        trial_dict['trial_status'] = soup.find('dt', text=re.compile('\\sRecruitment status\\s')).find_next('dd').text.strip()\n",
    "    else:\n",
    "        trial_dict['trial_status'] = None\n",
    "\n",
    "    if not soup.find('dt', text=re.compile('\\sTrial completion date\\s')) or soup.find('dt', text=re.compile('\\sTrial completion date\\s')).find_next('dd').text.strip() == 'empty':\n",
    "        trial_dict['completion_date'] = None\n",
    "    else:\n",
    "        trial_dict['completion_date'] = re.findall(re.compile('\\d{4}-\\d{2}-\\d{2}'), soup.find('dt', text=re.compile('\\sTrial completion date\\s')).find_next('dd').text.strip())[0]\n",
    "\n",
    "    if soup.find('h3', text=re.compile('Secondary Ids')):\n",
    "        trial_dict['secondary_ids'] = soup.find('h3', text=re.compile('Secondary Ids')).parent\n",
    "    else:\n",
    "        trial_dict['secondary_ids'] = None\n",
    "    \n",
    "    if soup.find('span', text='Last update:'):\n",
    "        trial_dict['last_updated'] = re.findall(r'\\d{4}-\\d{2}-\\d{2}', soup.find('span', text='Last update:').find_next().text)[0]\n",
    "    else:\n",
    "        trial_dict['last_updated'] = None\n",
    "    \n",
    "    emails = []\n",
    "    for x in soup.find_all('strong', text='Email'):\n",
    "        emails.append(x.find_next().text.strip())\n",
    "    trial_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    irct_list.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(irct_list).to_csv(save_path + 'irct_12jul_2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiCTR\n",
    "\n",
    "This scraper works poorly. You only get between 10-30 trials scraped before you run into the anti-dos blocks. I ran it multiple times over about a day and a half to gather the data on ChiCTR trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chictr_urls = df[df.source_register == 'ChiCTR'].web_address.to_list()\n",
    "\n",
    "chictr_trials = []\n",
    "\n",
    "tracker = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for u in tqdm(chictr_urls[tracker:]):\n",
    "    \n",
    "    soup=get_url(u)\n",
    "    \n",
    "    trial_dict = {}\n",
    "    \n",
    "    try:\n",
    "        test = soup.find('p', text = re.compile('\\w*Registration number\\w*'))\n",
    "    except AttributeError:\n",
    "        trial_dict['error'] = u\n",
    "        chictr_trials.append(trial_dict)\n",
    "        continue\n",
    "    \n",
    "    trial_dict['trial_id'] = soup.find('p', text = re.compile('\\w*Registration number\\w*')).find_next('td').text.strip()\n",
    "    \n",
    "    if len(re.findall(re.compile('\\d{4}-\\d{2}-\\d{2}'), soup.find('span', text='To').parent.text)) > 1:\n",
    "        trial_dict['comp_date'] = re.findall(re.compile('\\d{4}-\\d{2}-\\d{2}'), soup.find('span', text='To').parent.text)[1]\n",
    "    else:\n",
    "        trial_dict['comp_date'] = None\n",
    "            \n",
    "\n",
    "    trial_dict['trial_status'] = soup.find('p', text = re.compile('\\w*Recruiting status\\w*')).find_next('p').find_next('p').text.strip()\n",
    "    \n",
    "    trial_dict['last_updated'] = soup.find('p', text = re.compile('Date of Last Refreshed on\\w*')).find_next('td').text.strip()\n",
    "    \n",
    "    email_identifiers = [r'\\w*Applicant E-mail\\w*', r\"\\w*Study leader's fax\\w*\"]\n",
    "    \n",
    "    emails = []\n",
    "    for e in email_identifiers:\n",
    "        emails.append(soup.find('p', text=re.compile(e)).find_next('td').text.strip())\n",
    "    trial_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    \n",
    "    chictr_trials.append(trial_dict)\n",
    "    tracker +=1\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(chictr_trials).to_csv(save_path + 'chictr_jul21.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jRCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "jrct_urls = df[(df.source_register == 'JPRN') & (df.trialid.str.contains('jRCT'))].web_address.to_list()\n",
    "\n",
    "jrct_urls_eng = []\n",
    "\n",
    "for j in jrct_urls:\n",
    "    jrct_urls_eng.append(j[:24] + 'en-' + j[24:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jrct_urls_eng[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jrct_trials = []\n",
    "\n",
    "for j in tqdm(jrct_urls_eng):\n",
    "    j_dict = {}\n",
    "    soup = get_url(j)\n",
    "    \n",
    "    j_dict['trial_id'] = 'JPRN-' + j[-14:]\n",
    "    \n",
    "    j_dict['status'] = soup.find('label', text='Recruitment status').find_next().find_next().text\n",
    "    \n",
    "    j_dict['secondary_ids'] = soup.find('label', text='Secondary ID(s)').find_next().text\n",
    "    \n",
    "    j_dict['last_updated'] = pd.to_datetime(soup.find('label', text='Last modified on').find_next().text)\n",
    "    \n",
    "    emails = []\n",
    "    for x in soup.find_all('label', text='E-mail'):\n",
    "        emails.append(x.find_next().text.strip())\n",
    "    j_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    jrct_trials.append(j_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(jrct_trials).to_csv(save_path + 'jrct_13jul21.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REBEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebec_ids = df[df.source_register == 'REBEC'].trialid.to_list()\n",
    "\n",
    "rebec_url = 'https://ensaiosclinicos.gov.br/rg/{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = get_url(rebec_url.format(rebec_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebec_trials = []\n",
    "\n",
    "for r in tqdm(rebec_ids):\n",
    "    soup = get_url(rebec_url.format(r))\n",
    "    \n",
    "    r_t = {}\n",
    "    \n",
    "    r_t['trial_id'] = r\n",
    "    \n",
    "    try:\n",
    "        r_t['trial_status'] = soup.find('span', text=\"Study status:\").find_next().text.strip()\n",
    "    except AttributeError:\n",
    "        r_t['trial_status'] = None\n",
    "    \n",
    "    try:\n",
    "        r_t['last_enrollment'] = soup.find('span', text='Date last enrollment:').find_next().text.strip()\n",
    "    except AttributeError:\n",
    "        r_t['last_enrollment'] = None\n",
    "        \n",
    "    try:\n",
    "        r_t['last_updated'] = pd.to_datetime(soup.find('span', text='Last approval date\\xa0:').find_next('span').text.strip()[:10])\n",
    "    except AttributeError:\n",
    "        r_t['last_updated'] = None\n",
    "        \n",
    "    try:\n",
    "        emails = []\n",
    "        for x in soup.find_all('span', {'class':'email'}):\n",
    "            emails.append(x.text.strip())\n",
    "        r_t['emails'] = list(set(emails))\n",
    "    except AttributeError:\n",
    "        r_t['emails'] = None\n",
    "    \n",
    "    rebec_trials.append(r_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rebec_trials).to_csv(save_path + 'rebec_13jul_2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pactr_urls = df[df.source_register == 'PACTR'].web_address.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pactr_trials = []\n",
    "id_regex = re.compile(r'PACTR\\d{15}')\n",
    "\n",
    "for p in tqdm(pactr_urls):\n",
    "    soup = get_url(p)\n",
    "    p_dict = {}\n",
    "    \n",
    "    p_dict['trial_id'] = soup.find('td', text=re.compile(id_regex)).text\n",
    "    \n",
    "    p_dict['anticipated_comp'] = pd.to_datetime(soup.find('td', text='Anticipated date of last follow up').find_next('td').text)\n",
    "    \n",
    "    p_dict['actual_comp'] = pd.to_datetime(soup.find('td', text='Actual Last follow-up date').find_next('td').text)\n",
    "    \n",
    "    p_dict['trial_status'] = soup.find('td', text='Recruitment status').find_next('td').text\n",
    "    \n",
    "    if s.find('td', text='Changes to trial information').find_next('tr'):\n",
    "        p_dict['last_updated'] = pd.to_datetime(soup.find('td', text='Changes to trial information').find_next('tr').find_next('tr').find_all('td')[2].text)\n",
    "    else:\n",
    "        p_dict['last_updated'] = pd.to_datetime(soup.find('td', text='\\r\\n                  Date of Approval:\\r\\n                ').find_next('td').text)\n",
    "        \n",
    "    emails = []\n",
    "    for e in soup.find('td', text='CONTACT PEOPLE').parent.parent.find_all('b',text='Email'):\n",
    "        emails.append(e.find_next('tr').find_all('td')[2].text)\n",
    "    p_dict['emails'] = emails\n",
    "    \n",
    "    pactr_trials.append(p_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pactr_trials).to_csv(save_path + 'pactr_16jul_2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cris_urls = df[df.source_register == 'CRIS'].web_address.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cris_trials = []\n",
    "\n",
    "for c in tqdm(cris_urls):\n",
    "    soup = get_url(c)\n",
    "    c_dict = {}\n",
    "    \n",
    "    c_dict['trial_id'] = soup.find('th', text='\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t1. Background\\r\\n\\t\\t\\t\\t\\t\\t\\t').find_next('th').find_next('td').text.strip()\n",
    "    \n",
    "    c_dict['last_updated'] = pd.to_datetime(soup.find('p', {'class':'page_title_info'}).find_all('span')[-1].text)\n",
    "    \n",
    "    c_dict['trial_status'] = soup.find('th', text='\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t4. Status\\r\\n\\t\\t\\t\\t\\t\\t\\t').find_next('th').find_next('th').find_next('td').text.strip()\n",
    "    \n",
    "    if soup.find('th', text='Primary Completion Date').find_next('td'):\n",
    "        c_dict['pcd'] = pd.to_datetime(soup.find('th', text='Primary Completion Date').find_next('td').text.strip()[:10])\n",
    "    else:\n",
    "        c_dict['pcd'] = None\n",
    "    \n",
    "    if soup.find('th', text='Study Completion Date').find_next('td'):\n",
    "        c_dict['scd'] = pd.to_datetime(soup.find('th', text='Study Completion Date').find_next('td').text.strip()[:10])\n",
    "    else:\n",
    "        c_dict['scd'] = None\n",
    "        \n",
    "    c_dict['results_status'] = soup.find('th', text='\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t11. Study Results and Publication\\r\\n\\t\\t\\t\\t\\t\\t\\t').find_next('th').find_next('td').text.strip()\n",
    "    \n",
    "    cris_trials.append(c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cris_trials).to_csv(save_path + 'cris_16jul_2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPRN UMIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umin_urls = df[(df.source_register == 'JPRN') & (df.trialid.str.contains('UMIN'))].web_address.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umin_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umin_trials = []\n",
    "\n",
    "for u in tqdm(umin_urls):\n",
    "    u_dict = {}\n",
    "    soup=get_url(u)\n",
    "    \n",
    "    u_dict['trial_id'] = 'JPRN-' + soup.find('font', text='Unique ID issued by UMIN').find_next('td').text\n",
    "    \n",
    "    u_dict['trial_status'] = soup.find('font', text='Recruitment status').find_next('td').text\n",
    "    \n",
    "    u_dict['comp_date'] = pd.to_datetime(soup.find('font', text='Date trial data considered complete').find_next('td').text, errors='coerce')\n",
    "    \n",
    "    u_dict['last_updated'] = pd.to_datetime(soup.find('font', text='Last modified on').find_next('td').text, errors='coerce')\n",
    "    \n",
    "    emails = []\n",
    "    emails.append(soup.find('font', text='Research contact person').parent.parent.parent.parent.find('font', text='Email').find_next('td').text)\n",
    "    emails.append(soup.find('font', text='Public contact ').parent.parent.parent.parent.find('font', text='Email').find_next('td').text)\n",
    "    u_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    results = []\n",
    "    results_sec = soup.find('font', text='Result').parent.parent.parent.parent\n",
    "    \n",
    "    results.append(results_sec.find('font', text='Results').find_next('td'))\n",
    "\n",
    "    results.append(results_sec.find('font', text='URL related to results and publications').find_next('td'))\n",
    "    u_dict['results'] = results\n",
    "    \n",
    "    umin_trials.append(u_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(umin_trials).to_csv(save_path + 'umin_trials18jul21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPCEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpcec_urls = df[df.source_register == 'RPCEC'].web_address.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpcec_trials = []\n",
    "\n",
    "for rp in tqdm(rpcec_urls):\n",
    "    rp_dict = {}\n",
    "    soup = get_url(rp)\n",
    "    sopa = get_url(rp.replace('En', 'Sp').replace('en/trials', 'ensayos'))\n",
    "    \n",
    "    rp_dict['trial_id'] = soup.find('div', text=re.compile(r'\\s*Unique ID number:\\s*')).find_next().text.strip()\n",
    "    \n",
    "    rp_dict['trial_status'] = soup.find('div', text=re.compile(r'\\s*Recruitment status:\\s*')).find_next().text.strip()\n",
    "    \n",
    "    rp_dict['comp_date'] = pd.to_datetime(soup.find('div', text=re.compile(r'\\s*Study completion date:\\s*')).find_next().text.strip())\n",
    "    \n",
    "    rp_dict['last_updated'] = pd.to_datetime(soup.find('div', text = re.compile(r'\\s*Record Verification Date\\s*:\\s*')).find_next().text.strip())\n",
    "    \n",
    "    emails = []\n",
    "    for e in soup.find_all('div', text = re.compile(r'\\s*Email\\s*:\\s*')):\n",
    "        emails.append(e.find_next().text.strip())\n",
    "    rp_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    rp_dict['results'] = sopa.find('div', text=re.compile(r'\\s*Referencias:\\s*')).find_next().text.strip()\n",
    "    \n",
    "    rpcec_trials.append(rp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rpcec_trials).to_csv(save_path + 'rpcec_18jul21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "all,-language_info",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.3.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
