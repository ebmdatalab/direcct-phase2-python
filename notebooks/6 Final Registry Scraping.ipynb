{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent = str(Path(cwd).parents[0])\n",
    "sys.path.append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import post\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_url(url, parse='html'):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    response = get(url, verify = False, headers=headers)\n",
    "    html = response.content\n",
    "    if parse == 'html':\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    elif parse == 'xml':\n",
    "        soup = BeautifulSoup(html, \"xml\")\n",
    "    return soup\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "save_path = parent + '/data/final_registry_data_apr2022/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ClinicalTrials.gov', 'ChiCTR', 'EudraCT', 'UMIN-CTR', 'ISRCTN',\n",
       "       'NTR', 'ANZCTR', 'DRKS', 'IRCT', 'CRiS', 'JapicCTI', 'jRCT',\n",
       "       'REPEC', 'CTRI', 'LBCTR', 'ReBec', 'SLCTR', 'PACTR', 'RPCEC',\n",
       "       'TCTR'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(parent + '/data/2022-03-01_registrations.csv')\n",
    "df.registry.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClinicalTrials.gov    5925\n",
       "CTRI                  1218\n",
       "ChiCTR                 901\n",
       "IRCT                   609\n",
       "EudraCT                570\n",
       "DRKS                   352\n",
       "ISRCTN                 275\n",
       "UMIN-CTR               159\n",
       "ANZCTR                 152\n",
       "ReBec                  119\n",
       "NTR                    113\n",
       "jRCT                    70\n",
       "TCTR                    60\n",
       "PACTR                   55\n",
       "RPCEC                   49\n",
       "REPEC                   30\n",
       "CRiS                    19\n",
       "SLCTR                    6\n",
       "JapicCTI                 5\n",
       "LBCTR                    5\n",
       "Name: registry, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.registry.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'trn', 'registry'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClinicalTrials.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncts = df[df.registry == 'ClinicalTrials.gov'].trn.to_list()\n",
    "pubmed_res_str = 'Publications automatically indexed to this study by ClinicalTrials.gov Identifier (NCT Number):'\n",
    "url_str = 'https://clinicaltrials.gov/show/{}'\n",
    "\n",
    "ctgov_list = []\n",
    "\n",
    "\n",
    "for nct in tqdm(ncts):\n",
    "    soup = get_url(url_str.format(nct))\n",
    "    trial_dict = {}\n",
    "    \n",
    "    trial_dict['trial_id'] = nct\n",
    "    \n",
    "    #Completion Dates\n",
    "    if soup.find('span', {'data-term': \"Primary Completion Date\"}):\n",
    "        trial_dict['pcd'] = soup.find('span', {'data-term': \"Primary Completion Date\"}).find_next('td').text\n",
    "    else:\n",
    "        trial_dict['pcd'] = None\n",
    "    if soup.find('span', {'data-term': \"Study Completion Date\"}):\n",
    "        trial_dict['scd'] = soup.find('span', {'data-term': \"Study Completion Date\"}).find_next('td').text\n",
    "    else:\n",
    "        trial_dict['scd'] = None\n",
    "\n",
    "    #Tabular Results Status\n",
    "    if soup.find('li', {'id':'results'}):\n",
    "        trial_dict['tab_results'] = soup.find('li', {'id':'results'}).text.strip()\n",
    "    else:\n",
    "        trial_dict['tab_results'] = None\n",
    "\n",
    "    #Auto-linked results via PubMed\n",
    "    if soup.find('span', text=pubmed_res_str):\n",
    "        pm_linked = []\n",
    "        for x in soup.find('span', text=pubmed_res_str).find_next('div').find_all('div'):\n",
    "            pm_linked.append(x.text.strip())\n",
    "        trial_dict['linked_pubs'] = pm_linked\n",
    "    else:\n",
    "        trial_dict['linked_pubs'] = None\n",
    "\n",
    "    #Results citations provided by sponsor\n",
    "    if soup.find('span', text='Publications of Results:'):\n",
    "        spon_pubs = []\n",
    "        for x in soup.find('span', text='Publications of Results:').find_next('div').find_all('div'):\n",
    "            spon_pubs.append(x.text.strip())\n",
    "        trial_dict['spon_pubs'] = spon_pubs\n",
    "    else:\n",
    "        trial_dict['spon_pubs'] = None\n",
    "\n",
    "    #Trial Status:\n",
    "    if soup.find('span', {'data-term': 'Recruitment Status'}):\n",
    "        trial_dict['trial_status'] = soup.find('span', {'data-term': 'Recruitment Status'}).next_sibling.replace(':','').strip()\n",
    "    else:\n",
    "        trial_dict['trial_status'] = None\n",
    "\n",
    "    #Secondary IDs:\n",
    "    if soup.find('td', text='Other Study ID Numbers:'):\n",
    "        ids = []\n",
    "        for a in soup.find('td', text='Other Study ID Numbers:').find_next('td').text.split('\\n'):\n",
    "            if a.strip():\n",
    "                ids.append(a.strip())\n",
    "        trial_dict['secondary_ids'] = ids\n",
    "        \n",
    "    #Last updated date:\n",
    "    if soup.find('span', {'data-term': 'Last Update Posted'}):\n",
    "        trial_dict['last_updated'] = soup.find('span', {'data-term': 'Last Update Posted'}).next_sibling.replace(':','').strip()\n",
    "    else:\n",
    "        trial_dict['last_updated'] = None\n",
    "        \n",
    "    emails = []\n",
    "    for x in soup.select('a[href^=mailto]'):\n",
    "        emails.append(x.text)\n",
    "    trial_dict['emails'] = emails\n",
    "    \n",
    "    ctgov_list.append(trial_dict)\n",
    "    \n",
    "#Can be expanded for some covariates as needed but also can archive our full copy from the FDAAA TT \n",
    "#on the day of the scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgov_results = pd.DataFrame(ctgov_list)\n",
    "\n",
    "ctgov_results['pcd'] = pd.to_datetime(ctgov_results['pcd'])\n",
    "ctgov_results['scd'] = pd.to_datetime(ctgov_results['scd'])\n",
    "ctgov_results['last_updated'] = pd.to_datetime(ctgov_results['last_updated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgov_results.to_csv(save_path + 'ctgov_results_03apr2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISRCTN\n",
    "\n",
    "The ISRCTN doesn't allow scraping anymore and the API doesn't reliably return completion status so I downloaded the relevant fields for the entire ISRCTN in CSV format and will process from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isrctn_ids = df[df.registry == 'ISRCTN'].trn.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isrctn_df_full = pd.read_csv(save_path + 'ISRCTN_search_results_03apr2022_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isrctn_df_covid = isrctn_df[isrctn_df.ISRCTN.isin(isrctn_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isrctn_df_covid.to_csv(save_path + 'isrctn_03apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EUCTR\n",
    "IDs will be injested in form of EUCTR2020-000890-25\n",
    "We need to kill the EUCTR part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euctr_ids = [x[5:].replace('-Outside/EEA','') for x in df[df.registry == 'EudraCT'].trn.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euctr_trials = []\n",
    "\n",
    "for euctr_id in tqdm(euctr_ids):\n",
    "\n",
    "\n",
    "    search_url = 'https://www.clinicaltrialsregister.eu/ctr-search/search?query={}'\n",
    "    #First blank is the trial number, 2nd is the abbreviation for the protocol country\n",
    "    protocol_url = 'https://www.clinicaltrialsregister.eu/ctr-search/trial/{}/{}'\n",
    "\n",
    "    soup=get_url(search_url.format(euctr_id))\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    #trial id\n",
    "    trial_dict['trial_id'] = euctr_id\n",
    "\n",
    "    #Results status\n",
    "    if soup.find('span', {'class':'label'}, text='Trial results:'):\n",
    "        trial_dict['results_status'] = soup.find('span', {'class':'label'}, text='Trial results:').find_next().text\n",
    "    else:\n",
    "        euctr_trials.append(trial_dict)\n",
    "        continue\n",
    "\n",
    "    #Countries\n",
    "    country_list = []\n",
    "    for x in soup.find('span', text='Trial protocol:').parent.find_all('a'):\n",
    "        country_list.append(x.text)\n",
    "    trial_dict['countries'] = country_list\n",
    "\n",
    "    #Individual Protocol Data\n",
    "    #Completion dates\n",
    "    comp_dates = []\n",
    "    status_list = []\n",
    "    emails = []\n",
    "    for c in country_list:\n",
    "        psoup = get_url(protocol_url.format(euctr_id, c))\n",
    "        if psoup.find('td', text='Date of the global end of the trial'):\n",
    "            comp_dates.append(psoup.find('td', text='Date of the global end of the trial').find_next().text)\n",
    "        else:\n",
    "            comp_dates.append(None)\n",
    "    \n",
    "    #Trial status\n",
    "        if psoup.find('td', text='Trial Status:'):\n",
    "            status_list.append(psoup.find('td', text='Trial Status:').find_next('td').text.strip())\n",
    "        else:\n",
    "            status_list.append(None)\n",
    "\n",
    "    #secondary_ids\n",
    "        sid_dict = {}\n",
    "        if psoup.find('td', text='ISRCTN (International Standard Randomised Controlled Trial) Number'):\n",
    "            sid_dict['isrctn'] = psoup.find('td', text='ISRCTN (International Standard Randomised Controlled Trial) Number').find_next().text.strip()\n",
    "        if psoup.find('td', text='US NCT (ClinicalTrials.gov registry) number'):\n",
    "            sid_dict['nct_id'] = psoup.find('td', text='US NCT (ClinicalTrials.gov registry) number').find_next().text.strip()\n",
    "        if psoup.find('td', text=\"Sponsor's protocol code number\"):\n",
    "            sid_dict['spon_id'] = psoup.find('td', text=\"Sponsor's protocol code number\").find_next().text.strip()\n",
    "            \n",
    "    #emails\n",
    "        if psoup.find('td', text='E-mail'):\n",
    "            for x in psoup.find_all('td', text='E-mail'):\n",
    "                emails.append(x.find_next().text)\n",
    "            \n",
    "    \n",
    "    trial_dict['global_trial_end_dates'] =  comp_dates\n",
    "    trial_dict['status_list'] = status_list\n",
    "    trial_dict['emails'] = emails\n",
    "    if len(sid_dict) > 0:\n",
    "        trial_dict['secondary_ids'] = sid_dict\n",
    "    else:\n",
    "        trial_dict['secondary_ids'] = None\n",
    "    \n",
    "    euctr_trials.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(euctr_trials).to_csv(save_path + 'euctr_03apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRKS\n",
    "We can get DRKS trials via the URL in the ICTRP dataset like:\n",
    "https://www.drks.de/drks_web/navigate.do?navigationId=trial.HTML&TRIAL_ID=DRKS00021186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_it_exist(soup, element, e_class, n_e=False):\n",
    "    if not n_e:\n",
    "        location = soup.find(element, class_=e_class).text.strip()\n",
    "    elif n_e:\n",
    "        location = soup.find(element, class_=e_class).next_element.next_element.next_element.next_element.strip()\n",
    "    if location == '[---]*':\n",
    "        field = None\n",
    "    else:\n",
    "        field = location\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drks_url = 'https://www.drks.de/drks_web/navigate.do?navigationId=trial.HTML&TRIAL_ID={}'\n",
    "\n",
    "drks_ids = df[df.registry == 'DRKS'].trn.to_list()\n",
    "\n",
    "drks_trials = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in tqdm(drks_ids): \n",
    "    \n",
    "    soup = get_url(drks_url.format(d))\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    trial_dict['trial_id'] = soup.find('li', class_='drksId').next_element.next_element.next_element.next_element.strip()\n",
    "\n",
    "    history = get_url(f'https://www.drks.de/drks_web/navigate.do?navigationId=trial.history&TRIAL_ID={trial_dict[\"trial_id\"]}')\n",
    "    \n",
    "    st_class = ['state', 'deadline']\n",
    "    st_labels = ['recruitment_status', 'study_closing_date']\n",
    "    for lab, s_class in zip(st_labels, st_class):\n",
    "        trial_dict[lab] = does_it_exist(soup, 'li', s_class, n_e=True)\n",
    "\n",
    "    s_id_list = []\n",
    "    ul = soup.find('ul', class_='secondaryIDs').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        s_id_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            s_id_dict = {}\n",
    "            s_id_dict['id_type'] = u.next_element.next_element.next_element.strip().replace(':','')\n",
    "            li_t = u.next_element.next_element.next_element.next_element.strip()\n",
    "            li_t = re.sub('\\n', '|', li_t)\n",
    "            li_t = re.sub('\\s+', '', li_t).replace('(','').replace(')','')\n",
    "            id_info = li_t.split('|')\n",
    "            if len(id_info) > 1:   \n",
    "                s_id_dict['registry'] = id_info[1]\n",
    "                s_id_dict['secondary_id'] = id_info[0]\n",
    "            else:\n",
    "                s_id_dict['registry'] = None\n",
    "                s_id_dict['secondary_id'] = id_info[0]\n",
    "            s_id_list.append(s_id_dict)\n",
    "\n",
    "    trial_dict['secondary_ids'] = s_id_list\n",
    "\n",
    "    docs_list = []\n",
    "    ul = soup.find('ul', class_='publications').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        docs_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            doc_dict = {}\n",
    "            doc_dict['document_type'] = u.next_element.next_element.next_element.strip().replace(':','')\n",
    "            if u.find('a'):\n",
    "                doc_dict['link_to_document'] = u.find('a').get('href')\n",
    "            else:\n",
    "                doc_dict['link_to_document'] = None\n",
    "            docs_list.append(doc_dict)\n",
    "    trial_dict['results_publications_documents'] = docs_list\n",
    "    \n",
    "    trial_dict['last_updated'] = pd.to_datetime(history.find('tbody').find_next('td').text, format='%m-%d-%Y')\n",
    "    \n",
    "    emails = []\n",
    "    for x in soup.select('a[href^=mailto]'):\n",
    "        emails.append(x.text.replace(' at ', '@'))\n",
    "    trial_dict['emails'] = emails\n",
    "    \n",
    "    drks_trials.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(drks_trials).to_csv(save_path + 'drks_trials_03apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTRI\n",
    "\n",
    "Since CTRI URLs aren't linked to the trial id we need to join them in from the current ICTRP covid database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctri_trials = df[df.registry == 'CTRI']\n",
    "\n",
    "ictrp = pd.read_csv(parent + '/data/COVID19-web_31mar_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctri_merged = ctri_trials.merge(ictrp[['TrialID', 'web address']], how='left', left_on='trn', right_on='TrialID').drop('TrialID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctri_merged.at[71, 'web address'] = 'http://ctri.nic.in/Clinicaltrials/pmaindet2.php?trialid=45442'\n",
    "ctri_merged.at[72, 'web address'] = 'http://ctri.nic.in/Clinicaltrials/pmaindet2.php?trialid=11463'\n",
    "ctri_merged.at[73, 'web address'] = 'http://ctri.nic.in/Clinicaltrials/pmaindet2.php?trialid=16467'\n",
    "ctri_merged.at[819, 'web address'] = 'http://ctri.nic.in/Clinicaltrials/pmaindet2.php?trialid=49102'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctri_urls = ctri_merged['web address'].to_list()\n",
    "ctri_ids = ctri_merged['trn'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need a slightly more robust function to fetch trial data from the CTRI\n",
    "def get_ctri(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    tries=3\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            response = get(url, verify = False, headers=headers)\n",
    "        except ConnectionError as e:\n",
    "            if i < tries - 1:\n",
    "                sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctri_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for c,i in zip(tqdm(ctri_urls), ctri_ids):\n",
    "    try:\n",
    "        if c:\n",
    "            soup = get_ctri(c)\n",
    "\n",
    "            trial_dict = {}\n",
    "\n",
    "            trial_dict['trial_id'] = i\n",
    "\n",
    "            trial_dict['completion_date_india'] = soup.find('td', text = 'Date of Study Completion (India)').find_next('td').text.strip()\n",
    "\n",
    "            trial_dict['completion_date_global'] = soup.find('td', text = 'Date of Study Completion (Global)').find_next('td').text.strip()\n",
    "\n",
    "            trial_dict['recruitment_status_india'] = soup.find('b', text='Recruitment Status of Trial (India)').find_next('td').text.strip()\n",
    "\n",
    "            trial_dict['recruitment_status_global'] = soup.find('b', text='Recruitment Status of Trial (Global)').find_next('td').text.strip()\n",
    "\n",
    "            trial_dict['publication_details'] = soup.find('b', text='Publication Details').find_next('td').text.strip()\n",
    "\n",
    "            if soup.find('b', text = re.compile('Secondary IDs if Any')):\n",
    "                trial_dict['secondary_ids'] = soup.find('b', text = re.compile('Secondary IDs if Any')).parent.parent.find_all('tr')\n",
    "            else:\n",
    "                trial_dict['secondary_ids'] = None\n",
    "\n",
    "            trial_dict['last_updated'] = soup.find('b', text='Last Modified On:').find_next('td').text.strip()\n",
    "\n",
    "            emails = []\n",
    "            for x in soup.find_all('td', text='EmailÂ '):\n",
    "                emails.append(x.find_next('td').text.strip())\n",
    "            trial_dict['emails'] = list(set(emails))\n",
    "\n",
    "            ctri_list.append(trial_dict)\n",
    "\n",
    "            sleep(1)\n",
    "        else:\n",
    "            trial_dict = {}\n",
    "            trial_dict['trial_id'] = i\n",
    "            ctri_list.append(trial_dict)\n",
    "            continue\n",
    "    #Excuse the blind except here but was getting some weird errors. Easier to just check those by hand.\n",
    "    except:\n",
    "        trial_dict = {}\n",
    "        trial_dict['trial_id'] = i\n",
    "        ctri_list.append(trial_dict)\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ctri_list).to_csv(save_path + 'ctri_04jul2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANZCTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anzctr_url = 'https://anzctr.org.au/{}.aspx'\n",
    "\n",
    "anzctr_ids = df[df.registry == 'ANZCTR'].trn.to_list()\n",
    "\n",
    "anzctr_trials = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in tqdm(anzctr_ids):\n",
    "\n",
    "    soup = get_url(anzctr_url.format(u))\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    try:\n",
    "        trial_dict['trial_id'] = soup.find('span', {'id': 'ctl00_body_CXACTRNUMBER'}).text.replace('p','')\n",
    "    except AttributeError:\n",
    "        continue\n",
    "        \n",
    "\n",
    "    trial_dict['last_updated'] = soup.find('span', {'id': 'ctl00_body_CXUPDATEDATE'}).text\n",
    "\n",
    "    trial_dict['trial_status'] = soup.find('span', {'id': 'ctl00_body_CXRECRUITMENTSTATUS'}).text  \n",
    "    \n",
    "    anticipated_end_date = soup.find('span', {'id': 'ctl00_body_CXANTICIPATEDLASTVISITDATE'}).text\n",
    "\n",
    "    actual_end_date = soup.find('span', {'id': 'ctl00_body_CXACTUALLASTVISITDATE'}).text\n",
    "\n",
    "    if actual_end_date:\n",
    "        trial_dict['completion_date'] = actual_end_date\n",
    "    else:\n",
    "        trial_dict['completion_date'] = anticipated_end_date\n",
    "\n",
    "    secondary_ids = []\n",
    "\n",
    "    for x in soup.find_all('span', text=re.compile('Secondary ID \\[\\d*\\]')):\n",
    "        secondary_ids.append(x.find_next('div').span.text.strip())\n",
    "\n",
    "    trial_dict['secondary_ids'] = secondary_ids\n",
    "\n",
    "    results_dict = {}\n",
    "\n",
    "    if soup.find('div', {'id': 'ctl00_body_divNoResultsANZCTR'}):\n",
    "        trial_dict['results'] = None\n",
    "    else:\n",
    "        citations = []\n",
    "        for x in soup.find_all('span', text=re.compile('Publication date and citation/details \\[\\d*\\]')):\n",
    "            citations.append(x.find_next('div').span.text)\n",
    "\n",
    "        results_dict['citations'] = citations\n",
    "\n",
    "        if soup.find('a', {'id': 'ctl00_body_hyperlink_CXRESULTATTACHMENT'}):\n",
    "            results_dict['basic_reporting_doc'] = soup.find('a', {'id': 'ctl00_body_hyperlink_CXRESULTATTACHMENT'}).text\n",
    "\n",
    "    if results_dict:\n",
    "        trial_dict['results'] = results_dict\n",
    "    else:\n",
    "        trial_dict['results'] = None\n",
    "    \n",
    "    trial_dict['last_updated'] = pd.to_datetime(soup.find('span', {'id': 'ctl00_body_CXUPDATEDATE'}).text, format='%d/%m/%Y')\n",
    "    \n",
    "    emails = []\n",
    "    for x in soup.select('span[id$=_CXEMAIL]'):\n",
    "        if x.text and 'Email [' not in x.text:\n",
    "            emails.append(x.text)\n",
    "    trial_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    anzctr_trials.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anzctr_df = pd.DataFrame(anzctr_trials)\n",
    "anzctr_df.to_csv(save_path + 'anzctr_trials_03apr2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Easiest to just to this query and then filter\n",
    "\n",
    "rsp = post(\n",
    "    \"https://api.trialregister.nl/trials/public.trials/_msearch?\",\n",
    "    headers={\"Content-Type\": \"application/x-ndjson\", \"Accept\": \"application/json\"},\n",
    "    data='{\"preference\":\"results\"}\\n{\"query\":{\"match_all\":{}},\"size\":10000,\"_source\":{\"includes\":[\"*\"],\"excludes\":[]},\"sort\":[{\"id\":{\"order\":\"desc\"}}]}\\n',\n",
    ")\n",
    "results = rsp.json()\n",
    "hits = results[\"responses\"][0][\"hits\"][\"hits\"]\n",
    "records = [hit[\"_source\"] for hit in hits]\n",
    "\n",
    "all_keys = set().union(*(record.keys() for record in records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(all_keys)\n",
    "\n",
    "from datetime import date\n",
    "import csv\n",
    "\n",
    "def ntr_csv(save_path):\n",
    "    with open(save_path + 'ntr - ' + str(date.today()) + '.csv','w', newline = '', encoding='utf-8') as ntr_csv:\n",
    "        writer=csv.DictWriter(ntr_csv,fieldnames=labels)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only what we need from the NTR\n",
    "\n",
    "ntr_ids = df[df.registry == 'NTR'].trn.to_list()\n",
    "\n",
    "ntr_df = pd.read_csv(save_path + 'ntr - 2022-04-04.csv')\n",
    "\n",
    "covid_ntr = ntr_df[ntr_df.idFull.isin(ntr_ids)]\n",
    "\n",
    "covid_ntr[['idFull', 'status', 'dateStop', 'publications', 'idSource', 'isrctn', 'contact']].to_csv(save_path + 'ntr_covid_03apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRCT\n",
    "\n",
    "As with the CTRI, we need to pull in web adresses from the ICTRP as the URLS are not based on the trial ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irct_trials = df[df.registry == 'IRCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irct_merged = irct_trials.merge(ictrp[['TrialID', 'web address']], how='left', left_on='trn', right_on='TrialID').drop('TrialID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing missing urls\n",
    "irct_merged.at[0, 'web address'] = 'https://www.irct.ir/trial/46667'\n",
    "irct_merged.at[237, 'web address'] = 'https://www.irct.ir/trial/15589'\n",
    "irct_merged.at[239, 'web address'] = 'https://www.irct.ir/trial/38534'\n",
    "irct_merged.at[414, 'web address'] = 'https://www.irct.ir/trial/51802'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irct_urls = irct_merged['web address'].to_list()\n",
    "\n",
    "irct_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in tqdm(irct_urls):\n",
    "\n",
    "    soup=get_url(url)\n",
    "\n",
    "    trial_dict = {}\n",
    "    \n",
    "    if soup.find('span', text='IRCT registration number:'):\n",
    "        trial_dict['trial_id'] = soup.find('span', text='IRCT registration number:').find_next('strong').text.strip()\n",
    "    else:\n",
    "        trial_dict['trial_id'] = None\n",
    "\n",
    "    if soup.find('dt', text=re.compile('\\sRecruitment status\\s')):\n",
    "        trial_dict['trial_status'] = soup.find('dt', text=re.compile('\\sRecruitment status\\s')).find_next('dd').text.strip()\n",
    "    else:\n",
    "        trial_dict['trial_status'] = None\n",
    "\n",
    "    if not soup.find('dt', text=re.compile('\\sTrial completion date\\s')) or soup.find('dt', text=re.compile('\\sTrial completion date\\s')).find_next('dd').text.strip() == 'empty':\n",
    "        trial_dict['completion_date'] = None\n",
    "    else:\n",
    "        trial_dict['completion_date'] = re.findall(re.compile('\\d{4}-\\d{2}-\\d{2}'), soup.find('dt', text=re.compile('\\sTrial completion date\\s')).find_next('dd').text.strip())[0]\n",
    "\n",
    "    if soup.find('h3', text=re.compile('Secondary Ids')):\n",
    "        trial_dict['secondary_ids'] = soup.find('h3', text=re.compile('Secondary Ids')).parent\n",
    "    else:\n",
    "        trial_dict['secondary_ids'] = None\n",
    "    \n",
    "    if soup.find('span', text='Last update:'):\n",
    "        try:\n",
    "            trial_dict['last_updated'] = re.findall(r'\\d{4}-\\d{2}-\\d{2}', soup.find('span', text='Last update:').find_next().text)[0]\n",
    "        except IndexError:\n",
    "            trial_dict['last_updated'] = None\n",
    "    else:\n",
    "        trial_dict['last_updated'] = None\n",
    "    \n",
    "    emails = []\n",
    "    for x in soup.find_all('strong', text='Email'):\n",
    "        emails.append(x.find_next().text.strip())\n",
    "    trial_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    irct_list.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(irct_list).to_csv(save_path + 'irct_03apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jRCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jrct_trials = df[df.registry == 'jRCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jrct_merged = jrct_trials.merge(ictrp[['TrialID', 'web address']], how='left', left_on='trn', right_on='TrialID').drop('TrialID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jrct_merged[jrct_merged['web address'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing missing urls\n",
    "jrct_merged.at[31, 'web address'] = 'https://jrct.niph.go.jp/latest-detail/jRCT2031200174'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jrct_urls = jrct_merged['web address'].to_list()\n",
    "\n",
    "jrct_urls_eng = []\n",
    "\n",
    "for j in jrct_urls:\n",
    "    if isinstance(j, float):\n",
    "        jrct_urls_eng.append(None)\n",
    "    else:\n",
    "        jrct_urls_eng.append(j[:24] + 'en-' + j[24:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jrct_trials = []\n",
    "\n",
    "for j in tqdm(jrct_urls_eng):\n",
    "    j_dict = {}\n",
    "    \n",
    "    if not j:\n",
    "        j_dict['trial_id'] = 'JPRN-JRCT2031200252'\n",
    "        jrct_trials.append(j_dict)\n",
    "    else:\n",
    "        soup = get_url(j)\n",
    "\n",
    "        j_dict['trial_id'] = 'JPRN-' + j[-14:]\n",
    "\n",
    "        j_dict['status'] = soup.find('label', text='Recruitment status').find_next().find_next().text\n",
    "\n",
    "        j_dict['secondary_ids'] = soup.find('label', text='Secondary ID(s)').find_next().text\n",
    "\n",
    "        j_dict['last_updated'] = pd.to_datetime(soup.find('label', text='Last modified on').find_next().text)\n",
    "\n",
    "        emails = []\n",
    "        for x in soup.find_all('label', text='E-mail'):\n",
    "            emails.append(x.find_next().text.strip())\n",
    "        j_dict['emails'] = list(set(emails))\n",
    "\n",
    "        jrct_trials.append(j_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(jrct_trials).to_csv(save_path + 'jrct_04apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REBEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebec_ids = df[df.registry == 'ReBec'].trn.to_list()\n",
    "\n",
    "rebec_url = 'https://ensaiosclinicos.gov.br/rg/{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebec_trials = []\n",
    "\n",
    "for r in tqdm(rebec_ids):\n",
    "    soup = get_url(rebec_url.format(r))\n",
    "    \n",
    "    r_t = {}\n",
    "    \n",
    "    r_t['trial_id'] = r\n",
    "    \n",
    "    try:\n",
    "        r_t['trial_status'] = soup.find('span', text=\"Study status:\").find_next().text.strip()\n",
    "    except AttributeError:\n",
    "        r_t['trial_status'] = None\n",
    "    \n",
    "    try:\n",
    "        r_t['last_enrollment'] = soup.find('span', text='Date last enrollment:').find_next().text.strip()\n",
    "    except AttributeError:\n",
    "        r_t['last_enrollment'] = None\n",
    "        \n",
    "    try:\n",
    "        r_t['last_updated'] = pd.to_datetime(soup.find('span', text='Last approval date\\xa0:').find_next('span').text.strip()[:10])\n",
    "    except AttributeError:\n",
    "        r_t['last_updated'] = None\n",
    "        \n",
    "    try:\n",
    "        emails = []\n",
    "        for x in soup.find_all('span', {'class':'email'}):\n",
    "            emails.append(x.text.strip())\n",
    "        r_t['emails'] = list(set(emails))\n",
    "    except AttributeError:\n",
    "        r_t['emails'] = None\n",
    "    \n",
    "    rebec_trials.append(r_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rebec_trials).to_csv(save_path + 'rebec_04apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pactr_ids = df[df.registry == 'PACTR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pactr_merged = pactr_ids.merge(ictrp[['TrialID', 'web address']], how='left', left_on='trn', right_on='TrialID').drop('TrialID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pactr_urls = pactr_merged['web address'].to_list()\n",
    "\n",
    "pactr_trials = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_regex = re.compile(r'PACTR\\d{15}')\n",
    "\n",
    "for p in tqdm(pactr_urls):\n",
    "    soup = get_url(p)\n",
    "    p_dict = {}\n",
    "    \n",
    "    p_dict['trial_id'] = soup.find('td', text=re.compile(id_regex)).text\n",
    "    \n",
    "    p_dict['anticipated_comp'] = pd.to_datetime(soup.find('td', text='Anticipated date of last follow up').find_next('td').text)\n",
    "    \n",
    "    p_dict['actual_comp'] = pd.to_datetime(soup.find('td', text='Actual Last follow-up date').find_next('td').text)\n",
    "    \n",
    "    p_dict['trial_status'] = soup.find('td', text='Recruitment status').find_next('td').text\n",
    "    \n",
    "    if soup.find('td', text='Changes to trial information').find_next('tr'):\n",
    "        p_dict['last_updated'] = pd.to_datetime(soup.find('td', text='Changes to trial information').find_next('tr').find_next('tr').find_all('td')[2].text)\n",
    "    else:\n",
    "        p_dict['last_updated'] = pd.to_datetime(soup.find('td', text='\\r\\n                  Date of Approval:\\r\\n                ').find_next('td').text)\n",
    "        \n",
    "    emails = []\n",
    "    for e in soup.find('td', text='CONTACT PEOPLE').parent.parent.find_all('b',text='Email'):\n",
    "        emails.append(e.find_next('tr').find_all('td')[2].text)\n",
    "    p_dict['emails'] = emails\n",
    "    \n",
    "    pactr_trials.append(p_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pactr_trials).to_csv(save_path + 'pactr_04apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cris_ids = df[df.registry == 'CRiS']\n",
    "cris_merged = cris_ids.merge(ictrp[['TrialID', 'web address']], how='left', left_on='trn', right_on='TrialID').drop('TrialID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing missing urls\n",
    "cris_merged.at[4, 'web address'] = 'http://cris.nih.go.kr/cris/en/search/search_result_st01.jsp?seq=12657'\n",
    "cris_merged.at[5, 'web address'] = 'http://cris.nih.go.kr/cris/en/search/search_result_st01.jsp?seq=11087'\n",
    "cris_merged.at[6, 'web address'] = 'http://cris.nih.go.kr/cris/en/search/search_result_st01.jsp?seq=7441'\n",
    "cris_merged.at[7, 'web address'] = 'http://cris.nih.go.kr/cris/en/search/search_result_st01.jsp?seq=16278'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cris_urls = cris_merged['web address'].to_list()\n",
    "\n",
    "cris_trials = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm(cris_urls):\n",
    "    soup = get_url(c)\n",
    "    c_dict = {}\n",
    "    \n",
    "    c_dict['trial_id'] = soup.find('th', text='\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t1. Background\\r\\n\\t\\t\\t\\t\\t\\t\\t').find_next('th').find_next('td').text.strip()\n",
    "    \n",
    "    c_dict['last_updated'] = pd.to_datetime(soup.find('p', {'class':'page_title_info'}).find_all('span')[-1].text)\n",
    "    \n",
    "    c_dict['trial_status'] = soup.find('th', text='\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t4. Status\\r\\n\\t\\t\\t\\t\\t\\t\\t').find_next('th').find_next('th').find_next('td').text.strip()\n",
    "    \n",
    "    if soup.find('th', text='Primary Completion Date').find_next('td'):\n",
    "        c_dict['pcd'] = pd.to_datetime(soup.find('th', text='Primary Completion Date').find_next('td').text.strip()[:10])\n",
    "    else:\n",
    "        c_dict['pcd'] = None\n",
    "    \n",
    "    if soup.find('th', text='Study Completion Date').find_next('td'):\n",
    "        c_dict['scd'] = pd.to_datetime(soup.find('th', text='Study Completion Date').find_next('td').text.strip()[:10])\n",
    "    else:\n",
    "        c_dict['scd'] = None\n",
    "        \n",
    "    c_dict['results_status'] = soup.find('th', text='\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t11. Study Results and Publication\\r\\n\\t\\t\\t\\t\\t\\t\\t').find_next('th').find_next('td').text.strip()\n",
    "    \n",
    "    cris_trials.append(c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cris_trials).to_csv(save_path + 'cris_05apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPRN UMIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "umin_ids = df[df.registry == 'UMIN-CTR']\n",
    "umin_merged = umin_ids.merge(ictrp[['TrialID', 'web address']], how='left', left_on='trn', right_on='TrialID').drop('TrialID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing missing urls\n",
    "umin_merged.at[38, 'web address'] = 'https://upload.umin.ac.jp/cgi-open-bin/ctr_e/ctr_view.cgi?recptno=R000004846'\n",
    "umin_merged.at[39, 'web address'] = 'https://upload.umin.ac.jp/cgi-open-bin/ctr_e/ctr_view.cgi?recptno=R000006464'\n",
    "umin_merged.at[40, 'web address'] = 'https://upload.umin.ac.jp/cgi-open-bin/ctr_e/ctr_view.cgi?recptno=R000008394'\n",
    "umin_merged.at[41, 'web address'] = 'https://upload.umin.ac.jp/cgi-open-bin/ctr_e/ctr_view.cgi?recptno=R000024177'\n",
    "umin_merged.at[42, 'web address'] = 'https://upload.umin.ac.jp/cgi-open-bin/ctr_e/ctr_view.cgi?recptno=R000026773'\n",
    "umin_merged.at[43, 'web address'] = 'https://upload.umin.ac.jp/cgi-open-bin/ctr_e/ctr_view.cgi?recptno=R000030949'\n",
    "umin_merged.at[44, 'web address'] = 'https://upload.umin.ac.jp/cgi-open-bin/ctr_e/ctr_view.cgi?recptno=R000044045'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "umin_urls = umin_merged['web address'].to_list()\n",
    "\n",
    "umin_trials = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=get_url('https://upload.umin.ac.jp/cgi-open-bin/ctr_e/ctr_view.cgi?recptno=R000045268')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n2023\\nYear\\n08\\nMonth\\n31\\nDay\\n\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('font', text='Last follow-up date').find_next('td').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94876f9d85140c3acc2f75649dfe7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for u in tqdm(umin_urls):\n",
    "    u_dict = {}\n",
    "    soup=get_url(u)\n",
    "    \n",
    "    u_dict['trial_id'] = 'JPRN-' + soup.find('font', text='Unique ID issued by UMIN').find_next('td').text\n",
    "    \n",
    "    u_dict['trial_status'] = soup.find('font', text='Recruitment status').find_next('td').text\n",
    "    \n",
    "    u_dict['comp_date'] = soup.find('font', text='Last follow-up date').find_next('td').text\n",
    "    \n",
    "    u_dict['last_updated'] = pd.to_datetime(soup.find('font', text='Last modified on').find_next('td').text, errors='coerce')\n",
    "    \n",
    "    emails = []\n",
    "    emails.append(soup.find('font', text='Research contact person').parent.parent.parent.parent.find('font', text='Email').find_next('td').text)\n",
    "    emails.append(soup.find('font', text='Public contact ').parent.parent.parent.parent.find('font', text='Email').find_next('td').text)\n",
    "    u_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    results = []\n",
    "    results_sec = soup.find('font', text='Result').parent.parent.parent.parent\n",
    "    \n",
    "    results.append(results_sec.find('font', text='Results').find_next('td'))\n",
    "\n",
    "    results.append(results_sec.find('font', text='URL related to results and publications').find_next('td'))\n",
    "    u_dict['results'] = results\n",
    "    \n",
    "    umin_trials.append(u_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(umin_trials).to_csv(save_path + 'umin_trials_update.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPCEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpcec_ids = df[df.registry == 'RPCEC'].trn.to_list()\n",
    "\n",
    "rpcec_url_format = 'https://rpcec.sld.cu/en/trials/{}-En'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpcec_trials = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rp in tqdm(rpcec_ids):\n",
    "    rp_dict = {}\n",
    "    soup = get_url(rpcec_url_format.format(rp))\n",
    "    sopa = get_url(rpcec_url_format.format(rp).replace('En', 'Sp').replace('en/trials', 'ensayos'))\n",
    "    \n",
    "    rp_dict['trial_id'] = soup.find('div', text=re.compile(r'\\s*Unique ID number:\\s*')).find_next().text.strip()\n",
    "    \n",
    "    rp_dict['trial_status'] = soup.find('div', text=re.compile(r'\\s*Recruitment status:\\s*')).find_next().text.strip()\n",
    "    \n",
    "    rp_dict['comp_date'] = pd.to_datetime(soup.find('div', text=re.compile(r'\\s*Study completion date:\\s*')).find_next().text.strip())\n",
    "    \n",
    "    rp_dict['last_updated'] = pd.to_datetime(soup.find('div', text = re.compile(r'\\s*Record Verification Date\\s*:\\s*')).find_next().text.strip())\n",
    "    \n",
    "    emails = []\n",
    "    for e in soup.find_all('div', text = re.compile(r'\\s*Email\\s*:\\s*')):\n",
    "        emails.append(e.find_next().text.strip())\n",
    "    rp_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    rp_dict['results'] = sopa.find('div', text=re.compile(r'\\s*Referencias:\\s*')).find_next().text.strip()\n",
    "    \n",
    "    rpcec_trials.append(rp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rpcec_trials).to_csv(save_path + 'rpcec_05apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiCTR\n",
    "\n",
    "This scraper works poorly. You only get between 10-30 trials scraped before you run into the anti-dos blocks. I ran it multiple times over about a day and a half to gather the data on ChiCTR trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chictr_ids = df[df.registry == 'ChiCTR']\n",
    "\n",
    "chictr_merged = chictr_ids.merge(ictrp[['TrialID', 'web address']], how='left', left_on='trn', right_on='TrialID').drop('TrialID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chictr_urls = chictr_merged['web address'].to_list()\n",
    "\n",
    "chictr_trials = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for u in tqdm(chictr_urls[tracker:]):\n",
    "    \n",
    "    soup=get_url(u)\n",
    "    \n",
    "    trial_dict = {}\n",
    "    \n",
    "    try:\n",
    "        test = soup.find('p', text = re.compile('\\w*Registration number\\w*'))\n",
    "    except AttributeError:\n",
    "        trial_dict['error'] = u\n",
    "        chictr_trials.append(trial_dict)\n",
    "        continue\n",
    "    \n",
    "    trial_dict['trial_id'] = soup.find('p', text = re.compile('\\w*Registration number\\w*')).find_next('td').text.strip()\n",
    "    \n",
    "    if len(re.findall(re.compile('\\d{4}-\\d{2}-\\d{2}'), soup.find('span', text='To').parent.text)) > 1:\n",
    "        trial_dict['comp_date'] = re.findall(re.compile('\\d{4}-\\d{2}-\\d{2}'), soup.find('span', text='To').parent.text)[1]\n",
    "    else:\n",
    "        trial_dict['comp_date'] = None\n",
    "            \n",
    "\n",
    "    trial_dict['trial_status'] = soup.find('p', text = re.compile('\\w*Recruiting status\\w*')).find_next('p').find_next('p').text.strip()\n",
    "    \n",
    "    trial_dict['last_updated'] = soup.find('p', text = re.compile('Date of Last Refreshed on\\w*')).find_next('td').text.strip()\n",
    "    \n",
    "    email_identifiers = [r'\\w*Applicant E-mail\\w*', r\"\\w*Study leader's fax\\w*\"]\n",
    "    \n",
    "    emails = []\n",
    "    for e in email_identifiers:\n",
    "        emails.append(soup.find('p', text=re.compile(e)).find_next('td').text.strip())\n",
    "    trial_dict['emails'] = list(set(emails))\n",
    "    \n",
    "    \n",
    "    chictr_trials.append(trial_dict)\n",
    "    tracker +=1\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chictr_trials), tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(chictr_trials).to_csv(save_path + 'chictr_07apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tctr_ids = df[df.registry == 'TCTR'].trn.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tctr_url = 'http://www.thaiclinicaltrials.org/show/{}'\n",
    "tctr_api = 'https://www.thaiclinicaltrials.org/api/control/records/previews/{}?RID={}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['getRecordInfo.php', 'getRecordStatus.php', 'getRecordPubStudy.php', 'getRecordTitleSid.php', 'getRecordLocationA.php']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tctr_trials = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924e7b08837549cd8596f161ae8a7eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for t in tqdm(tctr_ids):\n",
    "    t_dict = {}\n",
    "    t_dict['trial_id'] = t\n",
    "    \n",
    "    get_id = re.search('PV_REC:\\d{4}', str(get_url(tctr_url.format(t))))[0][-4:]\n",
    "    \n",
    "    info = json.loads(get_url(tctr_api.format(files[0], get_id)).text)\n",
    "    t_dict['last_updated'] = info['records'][0]['last_update']\n",
    "    t_dict['trial_status'] = info['records'][0]['OverallStatusDesc']\n",
    "    \n",
    "    status = json.loads(get_url(tctr_api.format(files[1], get_id)).text)\n",
    "    t_dict['pcd'] = status['records'][0]['completion_date']\n",
    "    t_dict['scd'] = status['records'][0]['study_completion_date']\n",
    "    \n",
    "    pub = get_url(tctr_api.format(files[2], get_id)).text\n",
    "    if pub:\n",
    "        pub_d = json.loads(pub)\n",
    "        t_dict['linked_pubs'] = pub_d['records'][0]['url_link']\n",
    "    else:\n",
    "        t_dict['linked_pubs'] = None\n",
    "    \n",
    "    sid = get_url(tctr_api.format(files[3], get_id)).text\n",
    "    if sid:\n",
    "        sid_d = json.loads(sid)\n",
    "        \n",
    "        sids = []\n",
    "        for x in sid_d['records']:\n",
    "            sids.append(x['secondary_ids'])\n",
    "        \n",
    "        t_dict['secondary_ids'] = sids\n",
    "\n",
    "    else:\n",
    "        t_dict['secondary_ids'] = None\n",
    "        \n",
    "    contact = json.loads(get_url(tctr_api.format(files[4], get_id)).text)\n",
    "    contacts = []\n",
    "    try:\n",
    "        contacts.append(contact['records'][0]['sec_a_email'])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        contacts.append(contact['records'][0]['sec_a_bak_email'])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    t_dict['emails'] = contacts\n",
    "    \n",
    "    tctr_trials.append(t_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tctr_trials).to_csv(save_path + 'tctr_22apr_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling the trials from the registries that are either not easily scraped or very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registries = ['SLCTR', 'LBCTR', 'JapicCTI', 'REPEC']\n",
    "\n",
    "manual_trials = df[df.registry.isin(registries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_trials.to_csv(save_path + 'manual_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "all,-language_info"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
